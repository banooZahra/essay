{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeMlHm2l3pryL5ojDdET8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banooZahra/essay/blob/main/shenas_capsNet_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY2qU-O5NmcZ"
      },
      "source": [
        "## 1. Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ8Gv3FpmPf-"
      },
      "source": [
        "### 1.1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVWONqDLmhOw",
        "outputId": "aca27fdd-769d-4457-f1ae-6cf6f24270f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqGw820-mqE7"
      },
      "source": [
        "### 1.2. Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQi15hUIAdN1",
        "outputId": "01c65220-d94a-4f8e-aaa7-316122c47fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'type']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import math\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "types = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
        "         'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
        "types = [x.lower() for x in types]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words = [word.replace(\"'\", '') for word in stop_words]\n",
        "stop_words.append(\"type\")\n",
        "print(stop_words)\n",
        "\n",
        "def lemmatize(text):\n",
        "  for type_ in types: \n",
        "    text = text.replace(type_, '')\n",
        "  lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ') if (word not in stop_words)])\n",
        "  return lemmatized\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "  regex = re.compile('[%s]' % re.escape('|'))\n",
        "  text = regex.sub(\" \", text)\n",
        "  words = str(text).split()\n",
        "  words = [i.lower() + \" \" for i in words]\n",
        "  words = [i for i in words if not \"http\" in i]\n",
        "  words = \" \".join(words)\n",
        "  words = words.translate(words.maketrans('', '', string.punctuation))\n",
        "  words = re.sub(' +', ' ', words) \n",
        "  return words\n",
        "\n",
        "\n",
        "def preprocess_text(sentence):\n",
        "    # remove hyperlinks, hashtags, smileys, emojies\n",
        "    sentence = lemmatize(clean_text(sentence))\n",
        "    # Removing words with more than two consecutive characters\n",
        "    sentence = re.sub('\\\\S*(\\\\S)\\\\1\\\\1\\\\S*\\\\s?', ' ', sentence)\n",
        "    # Remove hyperlinks\n",
        "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
        "    # Remove punctuations and numbers\n",
        "    # sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    # sentence = re.sub('[^a-zA-Z.?!,]', ' ', sentence)\n",
        "    # Single character removal (except I)\n",
        "    # sentence = re.sub(r\"\\s+[a-zA-HJ-Z]\\s+\", ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    sentence = re.sub(r'\\|\\|\\|', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def load_essays_df(datafile):\n",
        "    with open(datafile, \"rt\") as csvf:\n",
        "        csvreader = csv.reader(csvf, delimiter=',', quotechar='\"')\n",
        "        first_line = True\n",
        "        df = pd.DataFrame(columns=[\"user\", \"text\", \"token_len\", \"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"])\n",
        "        for line in csvreader:\n",
        "            if first_line:\n",
        "                first_line = False\n",
        "                continue\n",
        "\n",
        "            text = line[1]\n",
        "            df = df.append({\"user\": line[0],\n",
        "                            \"text\": preprocess_text(text),\n",
        "                            \"token_len\": 0,\n",
        "                            \"EXT\": 1 if line[2].lower() == 'y' else 0,\n",
        "                            \"NEU\": 1 if line[3].lower() == 'y' else 0,\n",
        "                            \"AGR\": 1 if line[4].lower() == 'y' else 0,\n",
        "                            \"CON\": 1 if line[5].lower() == 'y' else 0,\n",
        "                            \"OPN\": 1 if line[6].lower() == 'y' else 0}, ignore_index=True)\n",
        "\n",
        "    print('EXT : ', df['EXT'].value_counts())\n",
        "    print('NEU : ', df['NEU'].value_counts())\n",
        "    print('AGR : ', df['AGR'].value_counts())\n",
        "    print('CON : ', df['CON'].value_counts())\n",
        "    print('OPN : ', df['OPN'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def essays_embeddings(datafile, tokenizer, token_length, mode):\n",
        "    targets = []\n",
        "    input_ids = []\n",
        "\n",
        "    df = load_essays_df(datafile)\n",
        "    cnt = 0\n",
        "\n",
        "    # sorting all essays in ascending order of their length\n",
        "    for ind in df.index:\n",
        "        tokens = tokenizer.tokenize(df['text'][ind])\n",
        "        df.at[ind, 'token_len'] = len(tokens)\n",
        "    \n",
        "    df.sort_values(by=['token_len', 'user'],inplace=True, ascending=True)\n",
        "    tmp_df = df['user']\n",
        "    tmp_df.to_csv('/content/personality-prediction/data/essays/author_id_order.csv', index_label='order')\n",
        "    print(df['token_len'].mean())\n",
        "\n",
        "    for ii in range(len(df)):\n",
        "        text = preprocess_text(df['text'][ii])\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "\n",
        "        if mode == 'normal' or mode == '512_head':\n",
        "            input_ids.append(\n",
        "                tokenizer.encode(tokens, add_special_tokens=True, max_length=token_length, pad_to_max_length=True))\n",
        "        elif mode == '512_tail':\n",
        "            input_ids.append(\n",
        "                tokenizer.encode(tokens[-(token_length - 2):], add_special_tokens=True, max_length=token_length,\n",
        "                                 pad_to_max_length=True))\n",
        "        elif mode == '256_head_tail':\n",
        "            input_ids.append(\n",
        "                tokenizer.encode(tokens[:(token_length - 1)] + tokens[-(token_length - 1):], add_special_tokens=True,\n",
        "                                 max_length=token_length, pad_to_max_length=True))\n",
        "\n",
        "        elif mode == 'docbert':\n",
        "            docmax_len = 2048\n",
        "            subdoc_len = 512\n",
        "            max_subdoc_num = docmax_len // subdoc_len\n",
        "            subdoc_tokens = [tokens[i:i + subdoc_len] for i in range(0, len(tokens), subdoc_len)][:max_subdoc_num]\n",
        "            # print(subdoc_tokens)\n",
        "            token_ids = [tokenizer.encode(x, add_special_tokens=True, max_length=token_length, pad_to_max_length=True)\n",
        "                         for x in subdoc_tokens]\n",
        "            # print(token_ids)\n",
        "            token_ids = np.array(token_ids).astype(int)\n",
        "\n",
        "            buffer_len = docmax_len // subdoc_len - token_ids.shape[0]\n",
        "            # print(buffer_len)\n",
        "            tmp = np.full(shape=(buffer_len, token_length), fill_value=0, dtype=int)\n",
        "            token_ids = np.concatenate((token_ids, tmp), axis=0)\n",
        "\n",
        "            input_ids.append(token_ids)\n",
        "\n",
        "        if (cnt < 3):\n",
        "            print(input_ids[-1])\n",
        "\n",
        "        targets.append([df['EXT'][ii], df['NEU'][ii], df['AGR'][ii], df['CON'][ii], df['OPN'][ii]])\n",
        "        cnt += 1\n",
        "\n",
        "    author_ids = np.array(df.index)\n",
        "    print('loaded all input_ids and targets from the data file!')\n",
        "    return author_ids, input_ids, targets\n",
        "\n",
        "\n",
        "def load_Kaggle_df(datafile):\n",
        "    with open(datafile, \"rt\", encoding='utf-8') as csvf:\n",
        "        csvreader = csv.reader(csvf, delimiter=',', quotechar='\"')\n",
        "        first_line = True\n",
        "        df = pd.DataFrame(columns=[\"user\", \"text\", \"E\", \"N\", \"F\", \"J\"])\n",
        "        for line in csvreader:\n",
        "            if first_line:\n",
        "                first_line = False\n",
        "                continue\n",
        "\n",
        "            text = line[1]\n",
        "            if int(line[3]) == 3559:\n",
        "              text = \"input text with size zero so this is literally fake text\"\n",
        "            df = df.append({\"user\": line[3],\n",
        "                            \"text\": preprocess_text(text),\n",
        "                            \"E\": 1 if line[0][0] == 'E' else 0,\n",
        "                            \"N\": 1 if line[0][1] == 'N' else 0,\n",
        "                            \"F\": 1 if line[0][2] == 'F' else 0,\n",
        "                            \"J\": 1 if line[0][3] == 'J' else 0, }, ignore_index=True)\n",
        "\n",
        "    print('E : ', df['E'].value_counts())\n",
        "    print('N : ', df['N'].value_counts())\n",
        "    print('F : ', df['F'].value_counts())\n",
        "    print('J : ', df['J'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def kaggle_embeddings(datafile, tokenizer, token_length):\n",
        "    hidden_features = []\n",
        "    targets = []\n",
        "    token_len = []\n",
        "    input_ids = []\n",
        "    author_ids = []\n",
        "\n",
        "    df = load_Kaggle_df(datafile)\n",
        "    cnt = 0\n",
        "    for ind in df.index:\n",
        "        \n",
        "        text = preprocess_text(df['text'][ind])\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        token_len.append(len(tokens))\n",
        "        token_ids = tokenizer.encode(tokens, add_special_tokens=True, max_length=token_length, pad_to_max_length=True)\n",
        "        if (cnt < 10):\n",
        "            print(tokens[:10])\n",
        "\n",
        "        input_ids.append(token_ids)\n",
        "        targets.append([df['E'][ind], df['N'][ind], df['F'][ind], df['J'][ind]])\n",
        "        author_ids.append(int(df['user'][ind]))\n",
        "        cnt += 1\n",
        "\n",
        "    print('average length : ', int(np.mean(token_len)))\n",
        "    author_ids = np.array(author_ids)\n",
        "\n",
        "    return author_ids, input_ids, targets\n",
        "\n",
        "\n",
        "def load_persian_df(datafile):\n",
        "    with open(datafile, \"rt\", encoding='utf-8') as csvf:\n",
        "        csvreader = csv.reader(csvf, delimiter=',', quotechar='\"')\n",
        "        first_line = True\n",
        "        user_id = 0\n",
        "        df = pd.DataFrame(columns=[\"user\", \"text\", \"E\", \"N\", \"F\", \"J\"])\n",
        "        for line in csvreader:\n",
        "            if first_line:\n",
        "                first_line = False\n",
        "                continue\n",
        "            text = line[1]\n",
        "            # text = line[0]\n",
        "            user_id += 1\n",
        "            df = df.append({\"user\": user_id,\n",
        "                            \"text\": text,\n",
        "                            \"E\": 1 if line[0][0] == 'E' else 0,\n",
        "                            \"N\": 1 if line[0][1] == 'N' else 0,\n",
        "                            \"F\": 1 if line[0][2] == 'F' else 0,\n",
        "                            \"J\": 1 if line[0][3] == 'J' else 0, }, ignore_index=True)\n",
        "\n",
        "    print('E : ', df['E'].value_counts())\n",
        "    print('N : ', df['N'].value_counts())\n",
        "    print('F : ', df['F'].value_counts())\n",
        "    print('J : ', df['J'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def persian_embeddings(datafile, tokenizer, token_length):\n",
        "    hidden_features = []\n",
        "    targets = []\n",
        "    token_len = []\n",
        "    input_ids = []\n",
        "    author_ids = []\n",
        "\n",
        "    df = load_persian_df(datafile)\n",
        "    cnt = 0\n",
        "    for ind in df.index:\n",
        "        \n",
        "        text = preprocess_text(df['text'][ind])\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        token_len.append(len(tokens))\n",
        "        token_ids = tokenizer.encode(tokens, add_special_tokens=True, max_length=token_length, pad_to_max_length=True)\n",
        "        if (cnt < 10):\n",
        "            print(tokens[:10])\n",
        "\n",
        "        input_ids.append(token_ids)\n",
        "        targets.append([df['E'][ind], df['N'][ind], df['F'][ind], df['J'][ind]])\n",
        "        author_ids.append(int(df['user'][ind]))\n",
        "        cnt += 1\n",
        "\n",
        "    print('average length : ', int(np.mean(token_len)))\n",
        "    author_ids = np.array(author_ids)\n",
        "\n",
        "    return author_ids, input_ids, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbb_GqJnAfiZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy.io import arff\n",
        "\n",
        "def read_and_process(path):\n",
        "    arff = open(path, 'r')\n",
        "    attributes = []\n",
        "    values = []\n",
        "    is_attr = True\n",
        "    arff.readline()\n",
        "    arff.readline()\n",
        "    while is_attr:\n",
        "        line = arff.readline()\n",
        "        if len(line.split()) == 0:\n",
        "            is_attr = False\n",
        "            continue\n",
        "        type = line.split()[0]\n",
        "        attr = ' '.join(line.split()[1:])\n",
        "        if type == \"@attribute\":\n",
        "            attributes.append(attr)\n",
        "        else:\n",
        "            is_attr = False\n",
        "    for line in arff.readlines():\n",
        "        if len(line.split(\",\")) < 10:\n",
        "            continue\n",
        "        else:\n",
        "            components = line.split(\",\")\n",
        "            values.append(components)\n",
        "            name = components[0].replace(\"\\'\", \"\").split(\"\\\\\\\\\")[-1]\n",
        "            values[-1][0] = name\n",
        "    df = pd.DataFrame(columns=attributes, data=values)\n",
        "    df['idx'] = [int(re.sub('id_', '', i)) for i in df[df.columns[0]]]\n",
        "    df = df.drop(df.columns[0], axis=1)\n",
        "    df = df.set_index(['idx'])\n",
        "    df = df.apply(pd.to_numeric, errors='coerce')\n",
        "    df = df.sort_index()\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_features(dir, dataset):\n",
        "    idx = 'id'\n",
        "    if dataset == 'kaggle':\n",
        "        drop_cols = ['BROWN-FREQ numeric', 'K-F-FREQ numeric', 'K-F-NCATS numeric', 'K-F-NSAMP numeric',\n",
        "                     'T-L-FREQ numeric', 'Extraversion numeric'\n",
        "            , '\\'Emotional stability\\' numeric', 'Agreeableness numeric', 'Conscientiousness numeric',\n",
        "                     '\\'Openness to experience\\' numeric']\n",
        "        mairesse = read_and_process(dir + dataset + '_mairesse_labeled.arff')\n",
        "        mairesse = mairesse.drop(drop_cols, axis=1)\n",
        "    elif dataset == 'essays':\n",
        "        idx = '#AUTHID'\n",
        "        mairesse = pd.read_csv(dir + dataset + '_mairesse_labeled.csv')\n",
        "        mairesse = mairesse.set_index(mairesse.columns[0])\n",
        "    nrc = pd.read_csv(dir + dataset + '_nrc.csv').set_index([idx])\n",
        "    # nrc = nrc.sort_values(by=['id'])\n",
        "    # nrc = nrc.drop(['id'], axis=1)\n",
        "    nrc_vad = pd.read_csv(dir + dataset + '_nrc_vad.csv').set_index([idx])\n",
        "    # nrc_vad = nrc_vad.sort_values(by=['id'])\n",
        "    # nrc_vad = nrc_vad.drop(['id'], axis=1)\n",
        "    # affectivespace = pd.read_csv(dir + 'essays_affectivespace.csv').set_index(['#AUTHID'])\n",
        "    # hourglass = pd.read_csv(dir + dataset + '_hourglass.csv').set_index([idx])\n",
        "    readability = pd.read_csv(dir + dataset + '_readability.csv').set_index([idx])\n",
        "\n",
        "    return [nrc, nrc_vad, readability, mairesse]\n",
        "\n",
        "\n",
        "def get_psycholinguist_data(dump_data, dataset, feature_flags):\n",
        "    features = load_features('/content/personality-prediction/data/' + dataset + '/psycholinguist_features/', dataset)\n",
        "\n",
        "    first = 1\n",
        "    for feature, feature_flag in zip(features, feature_flags):\n",
        "        if feature_flag:\n",
        "            if first:\n",
        "                df = feature\n",
        "                first = 0\n",
        "            else:\n",
        "                df = pd.merge(df, feature, left_index=True, right_index=True)\n",
        "    if dataset == 'essays':\n",
        "        labels = dump_data[['user', 'text', 'EXT', 'NEU', 'AGR', 'CON', 'OPN']]\n",
        "    if dataset == 'kaggle':\n",
        "        labels = dump_data[['user', 'text', 'E', 'N', 'F', 'J']]\n",
        "    labels = labels.set_index('user')\n",
        "    if dataset == 'kaggle':\n",
        "        labels.index = pd.to_numeric(labels.index, errors='coerce')\n",
        "        df.index = pd.to_numeric(df.index, errors='coerce')\n",
        "    merged = pd.merge(df, labels, left_index=True, right_index=True).fillna(0)\n",
        "    \n",
        "    return merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE7EVT_OA0pw",
        "outputId": "2aee0276-0dc3-48a6-ff7c-3e79b5d68f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'personality-prediction'...\n",
            "remote: Enumerating objects: 839, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 839 (delta 6), reused 2 (delta 0), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (839/839), 53.41 MiB | 16.09 MiB/s, done.\n",
            "Resolving deltas: 100% (499/499), done.\n",
            "Updating files: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yashsmehta/personality-prediction.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amASNqy7VNPH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "\n",
        "\n",
        "def get_inputs(op_dir, dataset):\n",
        "    \"\"\" Read data from pkl file and prepare for training. \"\"\"\n",
        "    file = open(op_dir + dataset + '.pkl', 'rb')\n",
        "    data = pickle.load(file)\n",
        "    orders, data_x, data_y = list(zip(*data))\n",
        "    file.close()\n",
        "\n",
        "    layer = 11\n",
        "    n_hl = 12\n",
        "    # alphaW is responsible for which BERT layer embedding we will be using\n",
        "    if (layer == 'all'):\n",
        "        alphaW = np.full([n_hl], 1 / n_hl)\n",
        "\n",
        "    else:\n",
        "        alphaW = np.zeros([n_hl])\n",
        "        alphaW[int(layer) - 1] = 1\n",
        "\n",
        "    # just changing the way data is stored (tuples of minibatches) and getting the output for the required layer of BERT using alphaW\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    author_ids = []\n",
        "\n",
        "    n_batches = len(data_y)\n",
        "    print(len(orders))\n",
        "\n",
        "    for ii in range(n_batches):\n",
        "        inputs.extend(np.einsum('k,kij->ij', alphaW, data_x[ii]))\n",
        "        targets.extend(data_y[ii])\n",
        "        author_ids.extend(orders[ii])\n",
        "\n",
        "    print('inputs shape: ', np.array(inputs).shape)\n",
        "    print('author_ids shape: ', np.array(author_ids).shape)\n",
        "\n",
        "    inputs = pd.DataFrame(np.array(inputs))\n",
        "    inputs['order'] = author_ids\n",
        "    inputs = inputs.set_index(['order'])\n",
        "    full_targets = pd.DataFrame(np.array(targets))\n",
        "    full_targets['order'] = author_ids\n",
        "    full_targets = full_targets.set_index(['order'])\n",
        "\n",
        "    if dataset == 'test':\n",
        "        trait_labels = ['E', 'N', 'F', 'J']\n",
        "        return trait_labels, inputs\n",
        "\n",
        "    elif dataset == 'essays':\n",
        "        dump_data = load_essays_df('/content/personality-prediction/data/essays/essays.csv')\n",
        "        trait_labels = ['EXT', 'NEU', 'AGR', 'CON', 'OPN']\n",
        "\n",
        "    elif dataset == 'kaggle':\n",
        "        dump_data = load_Kaggle_df('/content/personality-prediction/data/kaggle/kaggle.csv')\n",
        "        trait_labels = ['E', 'N', 'F', 'J']\n",
        "\n",
        "    data_other_features_df = get_psycholinguist_data(dump_data, dataset, feature_flags)\n",
        "    data_features_df = merge_features(inputs, data_other_features_df, trait_labels)\n",
        "\n",
        "    return trait_labels, data_features_df\n",
        "\n",
        "\n",
        "def merge_features(embedding, other_features, trait_labels):\n",
        "    \"\"\" Merge BERT and Psychologic features. \"\"\"\n",
        "    if dataset == 'essays':\n",
        "        orders = pd.read_csv('/content/personality-prediction/data/essays/author_id_order.csv').set_index(['order'])\n",
        "        df = pd.merge(embedding, orders, left_index=True, right_index=True).set_index(['user'])\n",
        "    else:\n",
        "        df = embedding\n",
        "    df = pd.merge(df, other_features, left_index=True, right_index=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "gPFDqFFD9bey",
        "outputId": "96407df8-5b15-4728-b332-d5916b387338"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-51a6f349f0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrc_vad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmairesse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfeature_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrc_vad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmairesse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mkaggle_trait_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkaggle_data_features_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-317ad770af28>\u001b[0m in \u001b[0;36mget_inputs\u001b[0;34m(op_dir, dataset)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"\"\" Read data from pkl file and prepare for training. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0morders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/document_xlm-roberta_features/kaggle.pkl'"
          ]
        }
      ],
      "source": [
        "dataset = \"kaggle\"\n",
        "op_dir = '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/document_xlm-roberta_features/'\n",
        "n_classes = 2\n",
        "features_dim = 123\n",
        "seed = 789\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "nrc, nrc_vad, readability, mairesse = [True, True, True, True]\n",
        "feature_flags = [nrc, nrc_vad, readability, mairesse]\n",
        "kaggle_trait_labels, kaggle_data_features_df = get_inputs(op_dir, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSwL3X0zWtnG"
      },
      "source": [
        "### concat with essays dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLcVTeMsjLzi"
      },
      "outputs": [],
      "source": [
        "dataset = \"essays\"\n",
        "essays_trait_labels, essays_data_features_df = get_inputs(op_dir, dataset)\n",
        "essays_data_features_df['E'] = essays_data_features_df['EXT']\n",
        "essays_data_features_df['N'] = essays_data_features_df['OPN']\n",
        "essays_data_features_df['F'] = essays_data_features_df['AGR']\n",
        "essays_data_features_df['J'] = essays_data_features_df['CON']\n",
        "essays_data_features_df = essays_data_features_df.drop('EXT', 1)\n",
        "essays_data_features_df = essays_data_features_df.drop('AGR', 1)\n",
        "essays_data_features_df = essays_data_features_df.drop('CON', 1)\n",
        "essays_data_features_df = essays_data_features_df.drop('OPN', 1)\n",
        "essays_data_features_df = essays_data_features_df.drop('NEU', 1)\n",
        "essays_data_features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIRkkGnsXl9Q"
      },
      "outputs": [],
      "source": [
        "kaggle_translate_df = pd.read_csv('/content/drive/MyDrive/personality_detection_XLM-RoBERTa/test/dataset/kaggle_translate.csv')\n",
        "essays_translate_df = pd.read_csv('/content/drive/MyDrive/personality_detection_XLM-RoBERTa/test/dataset/essays_translate.csv')\n",
        "kaggle_data_features_df['text'][0:8675] = kaggle_translate_df.translate\n",
        "kaggle_data_features_df.text[3559] = 'الکی'\n",
        "essays_data_features_df['text'][0:2467] = essays_translate_df.translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmqtK5Rmu84H"
      },
      "outputs": [],
      "source": [
        "test_data_features_df = pd.read_csv('/content/drive/MyDrive/personality_detection_XLM-RoBERTa/test/dataset/essays_translate_test.csv', index_col='order')\n",
        "essays_data_features_df = essays_data_features_df.drop(test_data_features_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSd_xm5jWUql"
      },
      "outputs": [],
      "source": [
        "# first = True\n",
        "# for e in range(0,2):\n",
        "#     for n in range(0,2):\n",
        "#         for f in range(0,2):\n",
        "#             for j in range(0,2):\n",
        "#                 res = essays_data_features_df.loc[(essays_data_features_df.E==e) & (essays_data_features_df.N==n) & (essays_data_features_df.F==f) & (essays_data_features_df.J==j)][10:20]\n",
        "#                 essays_data_features_df = essays_data_features_df.drop(res.index)\n",
        "#                 if first:\n",
        "#                     test_data_features_df = res\n",
        "#                     first = False\n",
        "#                 else:\n",
        "#                     test_data_features_df = test_data_features_df.append(res)\n",
        "# test_data_features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEzeljZwXxsP"
      },
      "outputs": [],
      "source": [
        "data_features_df = kaggle_data_features_df.append(essays_data_features_df)\n",
        "data_features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KXpMukJ2jzl"
      },
      "source": [
        "### 1.3. Set up GPU for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T73lLtFe2sYl"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfF1N73tzOlM"
      },
      "source": [
        "## 2. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wupNcAVzzVs"
      },
      "source": [
        "### 2.1. Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQTNmdR1yrf1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(texts):\n",
        "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
        "    \n",
        "    Args:\n",
        "        texts (List[str]): List of text data\n",
        "    \n",
        "    Returns:\n",
        "        tokenized_texts (List[List[str]]): List of list of tokens\n",
        "        word2idx (Dict): Vocabulary built from the corpus\n",
        "        max_len (int): Maximum sentence length\n",
        "    \"\"\"\n",
        "\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "\n",
        "    # Add <pad> and <unk> tokens to the vocabulary\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    # Building our vocab from the corpus starting from index 2\n",
        "    idx = 2\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "\n",
        "        # Add `tokenized_sent` to `tokenized_texts`\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "\n",
        "        # Add new token to `word2idx`\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "        # Update `max_len`\n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len\n",
        "\n",
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
        "    their index in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
        "            shape (N, max_len). It will the input of our CNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Pad sentences to max_len\n",
        "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_ids.append(input_id)\n",
        "    \n",
        "    return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htIbprNlxtY"
      },
      "source": [
        "Load Pretrained Vectors\n",
        "\n",
        "We will load the pretrain vectors for each tokens in our vocabulary. For tokens with no pretraiend vectors, we will initialize random word vectors with the same length and variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeH6TTFOd8W0"
      },
      "outputs": [],
      "source": [
        "# !pip install unrar\n",
        "# !unrar x \"/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/vecmap-glove-fasttext-720000.rar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBX8y7wgNiQh"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_vectors(word2idx, fname):\n",
        "    \"\"\"Load pretrained vectors and create embedding layers.\n",
        "    \n",
        "    Args:\n",
        "        word2idx (Dict): Vocabulary built from the corpus\n",
        "        fname (str): Path to pretrained vector file\n",
        "\n",
        "    Returns:\n",
        "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
        "            the size of word2idx and d is embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    # fname = '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/crawl-300d-2M.vec'\n",
        "    print(\"Loading pretrained vectors...\")\n",
        "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "    # Initilize random embeddings\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "\n",
        "    # Load pretrained vectors\n",
        "    count = 0\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfbGK9BTMWyu"
      },
      "source": [
        "### 2.2. Tokenize(persian fastText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHngi6OLuzpe",
        "outputId": "35d2ba37-efa1-4f66-ad79-29d43e272500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: /content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/farsi_fasttext/cc.fa.300.vec.gz: No such file or directory\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.1-py2.py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3127081 sha256=8a574884801a1f5b6eb62bae89e48d152bbe474a2a249c5e280fc5fa1ab9860a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!gunzip /content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/farsi_fasttext/cc.fa.300.vec.gz\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycIlxXaywIWa"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "def load_vectors(word2idx, fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "    count = 0\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ImIIvoOSWCT"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgLt4c-0iWKL"
      },
      "source": [
        "### 3.1. Create CapsNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep-ujyLmsehs"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "USE_CUDA = True\n",
        "embedding_dim = 300\n",
        "use_pretrained_embedding = True\n",
        "BATCH_SIZE = 50\n",
        "gru_len = 128\n",
        "Routings = 5\n",
        "Num_capsule = 16\n",
        "Dim_capsule = 2\n",
        "dropout_p = 0.25\n",
        "T_epsilon = 1e-5\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "class Embed_Layer(nn.Module):\n",
        "    def __init__(self, embedding_matrix=None, vocab_size=None, embedding_dim=300):\n",
        "        super(Embed_Layer, self).__init__()\n",
        "        self.encoder = nn.Embedding(vocab_size + 1, embedding_dim)\n",
        "        if use_pretrained_embedding:\n",
        "            self.encoder.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "    def forward(self, x, dropout_p=0.25):\n",
        "        return nn.Dropout(p=dropout_p)(self.encoder(x))\n",
        "\n",
        "\n",
        "class GRU_Layer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRU_Layer, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=300,\n",
        "                          hidden_size=gru_len,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "    def init_weights(self):\n",
        "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
        "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
        "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
        "        for k in ih:\n",
        "            nn.init.xavier_uniform_(k)\n",
        "        for k in hh:\n",
        "            nn.init.orthogonal_(k)\n",
        "        for k in b:\n",
        "            nn.init.constant_(k, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gru(x)\n",
        "\n",
        "\n",
        "class Caps_Layer(nn.Module):\n",
        "    def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
        "                 routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
        "                 activation='default', **kwargs):\n",
        "        super(Caps_Layer, self).__init__(**kwargs)\n",
        "\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_size = kernel_size\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'default':\n",
        "            self.activation = self.squash\n",
        "        else:\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "        if self.share_weights:\n",
        "            self.W = nn.Parameter(\n",
        "                nn.init.xavier_normal_(t.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
        "        else:\n",
        "            self.W = nn.Parameter(\n",
        "                t.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = t.matmul(x, self.W)\n",
        "        else:\n",
        "            print('add later')\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        input_num_capsule = x.size(1)\n",
        "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
        "                                      self.num_capsule, self.dim_capsule))\n",
        "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  # 转成(batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
        "        b = t.zeros_like(u_hat_vecs[:, :, :, 0])  # (batch_size,num_capsule,input_num_capsule)\n",
        "\n",
        "        for i in range(self.routings):\n",
        "            b = b.permute(0, 2, 1)\n",
        "            c = F.softmax(b, dim=2)\n",
        "            c = c.permute(0, 2, 1)\n",
        "            b = b.permute(0, 2, 1)\n",
        "            outputs = self.activation(t.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
        "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
        "            if i < self.routings - 1:\n",
        "                b = t.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
        "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
        "\n",
        "    # text version of squash, slight different from original one\n",
        "    def squash(self, x, axis=-1):\n",
        "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
        "        scale = t.sqrt(s_squared_norm + T_epsilon)\n",
        "        return x / scale\n",
        "\n",
        "\n",
        "class Dense_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Dense_Layer, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),  # num_capsule*dim_capsule -> num_classes\n",
        "            nn.Dropout(p=dropout_p, inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class Capsule_Main(nn.Module):\n",
        "    def __init__(self, eng_embedding_matrix=None, eng_vocab_size=None, fa_embedding_matrix=None, fa_vocab_size=None):\n",
        "        super(Capsule_Main, self).__init__()\n",
        "        self.eng_embed_layer = Embed_Layer(eng_embedding_matrix, eng_vocab_size)\n",
        "        self.fa_embed_layer = Embed_Layer(fa_embedding_matrix, fa_vocab_size)\n",
        "        self.gru_layer = GRU_Layer()\n",
        "        self.gru_layer.init_weights()\n",
        "        self.caps_layer = Caps_Layer()\n",
        "        self.eng_dense_layer1 = Dense_Layer(Num_capsule * Dim_capsule + 891, 16)# + 891\n",
        "        self.fa_dense_layer1 = Dense_Layer(Num_capsule * Dim_capsule + 891, 16)# + 891\n",
        "        self.dense_layer2 = Dense_Layer(16, 2)\n",
        "\n",
        "    def forward(self, content, features, lang):\n",
        "        if lang == 'eng':\n",
        "            content1 = self.eng_embed_layer(content)\n",
        "        elif lang == 'fa':\n",
        "            content1 = self.fa_embed_layer(content)\n",
        "        content2, _ = self.gru_layer(content1)  \n",
        "        # 这个输出是个tuple，一个output(batch_size, seq_len, num_directions * hidden_size)，一个hn\n",
        "        content3 = self.caps_layer(content2)\n",
        "        content4 = torch.flatten(content3, start_dim=1)\n",
        "        content4 = torch.cat([content4, features], dim=-1)\n",
        "        if lang == 'eng':\n",
        "            content5 = self.eng_dense_layer1(content4)\n",
        "        elif lang == 'fa':\n",
        "            content5 = self.fa_dense_layer1(content4)\n",
        "        content5 = F.relu(content5)\n",
        "        output = self.dense_layer2(content5)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf9xEQR-yO_U"
      },
      "source": [
        "### 3.2. Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3PKPLc_sp8M"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def initilize_model():\n",
        "  \n",
        "    # Instantiate CNN model\n",
        "    capnet = Capsule_Main(eng_embeddings, len(eng_embeddings)-1, fa_embeddings, len(fa_embeddings)-1)\n",
        "    \n",
        "    # Send model to `device` (GPU/CPU)\n",
        "    capnet.to(device)\n",
        "\n",
        "    # Instantiate Adadelta optimizer\n",
        "    # optimizer = optim.Adadelta(capnet.parameters(), lr=0.1, rho=0.95)\n",
        "    # optimizer = optim.SGD(capnet.parameters(), lr=0.001, momentum=0.9)\n",
        "    # optimizer = optim.ASGD(capnet.parameters(), lr=0.0001, lambd=0.001, alpha=0.5, t0=1000000.0, weight_decay=0)\n",
        "    # optimizer = optim.AdamW(capnet.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    optimizer = optim.Adam(capnet.parameters(), lr=0.2e-4)#, eps=1e-08, weight_decay=0.01)\n",
        "\n",
        "    return capnet, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyWHu4sr55lU"
      },
      "source": [
        "### 3.3. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDg_46h2K3um"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "# Specify loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_features, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_features.float())\n",
        "            logits = F.softmax(logits)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if val_dataloader is not None:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_loss = []\n",
        "    label_predictions = []\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_features, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_features.float())\n",
        "            logits = F.softmax(logits)\n",
        "\n",
        "        # Compute loss and accumulate the loss values\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        label_predictions.extend(logits[1].cpu().detach().tolist())\n",
        "        # print(label_predictions)\n",
        "        pred = torch.argmax(logits.cpu().detach(), dim=1).flatten()\n",
        "        target = np.round(b_labels.cpu().detach())          \n",
        "        y_pred.extend(pred.tolist())\n",
        "        y_true.extend(target.tolist())\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    accuracy = accuracy_score(y_true,y_pred) * 100\n",
        "    val_loss = np.mean(val_loss)\n",
        "\n",
        "    return val_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHgnBNo1ino_"
      },
      "source": [
        "## 4. Evaluation and Testing Model (K-Fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJkI7QcYjtjl"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSJfqPEWmneq"
      },
      "outputs": [],
      "source": [
        "trait_labels = kaggle_trait_labels\n",
        "n_splits = 10\n",
        "set_seed(seed)\n",
        "expdata = {}\n",
        "expdata['train_acc'], expdata['val_acc'], expdata['val_loss'], expdata['trait'], expdata['fold'] = [], [], [], [], []\n",
        "\n",
        "for trait_idx in range(len(trait_labels)):\n",
        "#   if trait_idx != 0: continue\n",
        "  print(trait_labels[trait_idx])\n",
        "  print(\"before balancing\")\n",
        "  df = data_features_df.rename(columns={trait_labels[trait_idx]: 'toxic'})\n",
        "  label_0 = df.loc[df.toxic == 0]\n",
        "  label_1 = df.loc[df.toxic == 1]\n",
        "  print(str(len(label_1)) + \" , \" + str(len(label_0)))\n",
        "  min_length = min(len(label_1), len(label_0))\n",
        "  print(min_length)\n",
        "\n",
        "  df = pd.concat([\n",
        "      df.query('toxic==0').sample(n=min_length, random_state=seed),\n",
        "      df.query('toxic==1').sample(n=min_length, random_state=seed)])\n",
        "\n",
        "  label_0 = df.loc[df.toxic == 0]\n",
        "  label_1 = df.loc[df.toxic == 1]\n",
        "  print(\"after balancing\")\n",
        "  print(str(len(label_1)) + \" , \" + str(len(label_0)))\n",
        "  inputs = df[df.columns[:-1*(len(trait_labels)+1)]].values\n",
        "  targets = df['toxic'].values\n",
        "\n",
        "  # Tokenize, build vocabulary, encode tokens\n",
        "  print(\"Tokenizing...\\n\")\n",
        "  df['text'], word2idx, max_len = tokenize(df['text'])\n",
        "\n",
        "  # Load pretrained vectors\n",
        "#   embeddings = load_pretrained_vectors(word2idx)\n",
        "#   embeddings = torch.tensor(embeddings)\n",
        "\n",
        "#   Load fastText pretrained vectors\n",
        "  eng_embeddings = load_pretrained_vectors(word2idx, '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/crawl-300d-2M.vec')\n",
        "  eng_embeddings = torch.tensor(eng_embeddings)\n",
        "\n",
        "  fa_embeddings = load_vectors(word2idx, '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/farsi_fasttext/cc.fa.300.vec')\n",
        "  fa_embeddings = torch.tensor(fa_embeddings)\n",
        "\n",
        "  expdata['trait'].extend([trait_labels[trait_idx]] * n_splits)\n",
        "  expdata['fold'].extend(np.arange(1, n_splits + 1))\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=n_splits, shuffle=False)\n",
        "  k = 0\n",
        "  for train_index, test_index in skf.split(inputs, targets):\n",
        "\n",
        "    train_df = df.iloc[train_index]\n",
        "    val_df = df.iloc[test_index]\n",
        "\n",
        "    train_input_ids = encode(train_df['text'], word2idx, max_len)\n",
        "    train_inputs = np.array(train_input_ids)\n",
        "    y_train = np.array(train_df['toxic'])\n",
        "    f_train = np.array(train_df[train_df.columns[:(-1*(len(trait_labels)+1))]].values)\n",
        "\n",
        "    val_inout_ids = encode(val_df['text'], word2idx, max_len)\n",
        "    val_inputs = np.array(val_inout_ids)\n",
        "    y_val = np.array(val_df['toxic'])\n",
        "    f_val = np.array(val_df[train_df.columns[:(-1*(len(trait_labels)+1))]].values)\n",
        "\n",
        "    # Convert all data types to torch.Tensor\n",
        "    train_inputs = torch.tensor(train_inputs)\n",
        "    val_inputs = torch.tensor(val_inputs)\n",
        "    train_features = torch.tensor(f_train, dtype=torch.float32)\n",
        "    val_features = torch.tensor(f_val, dtype=torch.float32)\n",
        "    train_labels = torch.tensor(y_train)\n",
        "    val_labels = torch.tensor(y_val)\n",
        "\n",
        "    batch_size = 50\n",
        "\n",
        "    # Create the DataLoader for our training set\n",
        "    train_data = TensorDataset(train_inputs, train_features, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data = TensorDataset(val_inputs, val_features, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    # CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\n",
        "    set_seed(seed)\n",
        "    capsNet, optimizer = initilize_model()\n",
        "    train(capsNet, optimizer, train_dataloader, val_dataloader, epochs=15)\n",
        "\n",
        "    _, train_accuracy = evaluate(capsNet, train_dataloader)\n",
        "    expdata['train_acc'].append(train_accuracy)\n",
        "    print(\"train accuracy:\")\n",
        "    print(train_accuracy)\n",
        "\n",
        "    val_loss, val_accuracy = evaluate(capsNet, val_dataloader)\n",
        "    expdata['val_loss'].append(val_loss)\n",
        "    expdata['val_acc'].append(val_accuracy)\n",
        "    print(\"val accuracy:\")\n",
        "    print(val_accuracy)\n",
        "    k += 1\n",
        "    print(k)\n",
        "\n",
        "  # if trait_idx == 3: break\n",
        "\n",
        "df = pd.DataFrame.from_dict(expdata)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVDVkTc_nS5l"
      },
      "outputs": [],
      "source": [
        "df.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJGQKsNSkNgd"
      },
      "source": [
        "# 5. Evaluation on Hole Data and Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXUKWtdMYaVj"
      },
      "outputs": [],
      "source": [
        "trait_labels = kaggle_trait_labels\n",
        "n_splits = 1\n",
        "set_seed(seed)\n",
        "expdata = {}\n",
        "expdata['train_acc'], expdata['trait'], expdata['fold'] = [], [], []\n",
        "\n",
        "for trait_idx in range(len(trait_labels)):\n",
        "  if trait_idx == 0: continue\n",
        "  print(trait_labels[trait_idx])\n",
        "  print(\"before balancing\")\n",
        "  df = data_features_df.rename(columns={trait_labels[trait_idx]: 'toxic'})\n",
        "  label_0 = df.loc[df.toxic == 0]\n",
        "  label_1 = df.loc[df.toxic == 1]\n",
        "  print(str(len(label_1)) + \" , \" + str(len(label_0)))\n",
        "  min_length = min(len(label_1), len(label_0))\n",
        "  print(min_length)\n",
        "\n",
        "  df = pd.concat([\n",
        "      df.query('toxic==0').sample(n=min_length, random_state=seed),\n",
        "      df.query('toxic==1').sample(n=min_length, random_state=seed)])\n",
        "\n",
        "  label_0 = df.loc[df.toxic == 0]\n",
        "  label_1 = df.loc[df.toxic == 1]\n",
        "  print(\"after balancing\")\n",
        "  print(str(len(label_1)) + \" , \" + str(len(label_0)))\n",
        "  inputs = df[df.columns[:-1*(len(trait_labels)+1)]].values\n",
        "  targets = df['toxic'].values\n",
        "\n",
        "  # Tokenize, build vocabulary, encode tokens\n",
        "  print(\"Tokenizing...\\n\")\n",
        "  df['text'], word2idx, max_len = tokenize(df['text'])\n",
        "\n",
        "  # Load vec map pretrained vectors\n",
        "#   eng_embeddings = load_pretrained_vectors(word2idx, '/content/SRC-EN.vec-718000.txt')\n",
        "#   eng_embeddings = torch.tensor(eng_embeddings)\n",
        "\n",
        "#   fa_embeddings = load_pretrained_vectors(word2idx, '/content/TGT-FA.vec-718000.txt')\n",
        "#   fa_embeddings = torch.tensor(fa_embeddings)\n",
        "\n",
        "  # Load fastText pretrained vectors\n",
        "  eng_embeddings = load_pretrained_vectors(word2idx, '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/crawl-300d-2M.vec')\n",
        "  eng_embeddings = torch.tensor(eng_embeddings)\n",
        "\n",
        "  fa_embeddings = load_vectors(word2idx, '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/farsi_fasttext/cc.fa.300.vec')\n",
        "  fa_embeddings = torch.tensor(fa_embeddings)\n",
        "\n",
        "\n",
        "  expdata['trait'].extend([trait_labels[trait_idx]] * n_splits)\n",
        "  expdata['fold'].extend(np.arange(1, n_splits + 1))\n",
        "\n",
        "  train_df = df\n",
        "\n",
        "  train_input_ids = encode(train_df['text'], word2idx, max_len)\n",
        "  train_inputs = np.array(train_input_ids)\n",
        "  y_train = np.array(train_df['toxic'])\n",
        "  f_train = np.array(train_df[train_df.columns[:(-1*(len(trait_labels)+1))]].values)\n",
        "\n",
        "\n",
        "  # Convert all data types to torch.Tensor\n",
        "  train_inputs = torch.tensor(train_inputs)\n",
        "  train_features = torch.tensor(f_train, dtype=torch.float32)\n",
        "  train_labels = torch.tensor(y_train)\n",
        "\n",
        "  batch_size = 50\n",
        "\n",
        "   # Create the DataLoader for our training set\n",
        "  train_data = TensorDataset(train_inputs, train_features, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "#   CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\n",
        "  set_seed(seed)\n",
        "#   capsNet, optimizer = initilize_model()\n",
        "#   train(capsNet, optimizer, train_dataloader, train_dataloader, epochs=10)\n",
        "  cnn_non_static, optimizer = initilize_model(pretrained_embedding=fa_embeddings,\n",
        "                                                freeze_embedding=False,\n",
        "                                                learning_rate=0.1,\n",
        "                                                dropout=0.5)\n",
        "  train(cnn_non_static, optimizer, train_dataloader, train_dataloader, epochs=20)\n",
        "\n",
        "  _, train_accuracy = evaluate(cnn_non_static, train_dataloader)\n",
        "  expdata['train_acc'].append(train_accuracy)\n",
        "  print(\"train accuracy:\")\n",
        "  print(train_accuracy)\n",
        "\n",
        "  torch.save(cnn_non_static, '/content/drive/MyDrive/personality_detection_XLM-RoBERTa/fastText/models/cnn_(kaggle+essays-test)_fa_fast_xlm_' + trait_labels[trait_idx] + '.pth')\n",
        "\n",
        "  # if trait_idx == 3: break\n",
        "\n",
        "df = pd.DataFrame.from_dict(expdata)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp_Ins2MrirW",
        "outputId": "46026935-0922-4331-fbe6-cef0ecd3fa0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "train_acc    67.211658\n",
              "fold          1.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.mean()"
      ]
    }
  ]
}